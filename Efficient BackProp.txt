Efficient BackProp:

Рассматриваются особенности обучения нейронных сетей методом градиентного спуска.

0. Обучение нейронных сетей требует подбора большого числа параметров: начиная от архитектуры и заканчивая разбиением на обучающую и тестовую выборки. Существуют некотороые эвристические подходы к их выбору.

1. Ошибка обобщения (Generalization error) модели обычно представляется в виде суммы двух ошибок: смещения и дисперсии.
Смещение (Bias) модели - это усредненное по возможным обучающим выборкам отклонение ответов модели от правильных ответов.
Дисперсия (Variance) модели - это дисперсия отклонений ответов модели от правильных ответов при обучении на всех возможных обучающих выборках.
Если модель "недообучается", то первая компонента ошибки будет большой (High Bias), т.к. модель "не выучила" функцию и будет ошибаться на всех примерах примерно одинаково. 
Если модель "переобучается", то большой будет вторая компонента (High Variance), т.к. модель выучивает функцию, хорошо описывающую один набор данных, и настроенную на его шум. Т.к. в разных наборах данных шум различен, то ответы на незнакомых примерах будут "отклоняться" в любую сторону.

Существуют общие рекомендации для улучшения обобщающей способности моделей (регуляризация, ранняя остановка, ...), но если модель "слабая" (либо мало параметров, либо не проводился подбор модели), то это не поможет.

2. Метод обратного распространения ошибки.
Обучение сетей с большим количеством слоёв (=> весов) может быть очень медленным из-за сложной формы функции потерь (негладкость, невыпуклость, много локальных минимумов), и нет единого алгоритма, обещающего (!) быструю (!!) сходимость к глобальному (!!!) минимуму.

* Обучение пакетами (batch) vs. он-лайн обучение
Можно либо обновлять веса 1 раз за эпоху (1 эпоха = просмотр всех примеров из датасета и суммирование их вкладов в итоговую функцию потерь), тогда это называется Batch learning. Можно обновлять веса после каждого просмотренного примера, это On-line learning (или Stochastic learning). 
Плюсы SL:
+ Гораздо более быстро обучение
+ Есть вероятность попасть в более глубокий минимум функции потерь из-за шумов 
+ Можно подстраиваться под изменяющееся распределение данных
+ Можно отслеживать изменения "в реальном времени"
Плюсы BL:
+ Всегда сойдемся куда-нибудь (но из-за шума наверняка не в глобальный минимум)
+ Некоторые развитые методы градиентного спуска работают только с BL (см. conjugate gradient)
+ Проще анализ изменения весов и скорости сходимости (????)
Обычно в BL при приближении к минимуму ошибка начинает скакать вокруг да около, поэтому есть предложение уменьшать со временем коэффициент перед градиентом в формуле для обновления весов - так можно регулировать шаг градиентного спуска.

Есть путь, совмещающий BL и SL - это обучение по мини-пакетам (Mini-batch learning). За один раз просматриваем некоторое количество примеров из всего набора данных и обновляем веса. Обучение будет быстрым, менее подверженным к шуму, ...

* Перемешивание примеров
Модель обучается быстрее всего на наиболее неожиданных для себя примерах. Порядок предъявления примеров в режиме BL неважен, но имеет очень большое значение при обучении по мини-батчам. Чем более разнородные примеры мы последовательно показываем модели, тем лучше для обучения.
+ Перемешивать примеры так, чтобы последовательно предъявлялись примеры из разных классов
+ Чаще предъявлять примеры, на которых модель ошибается сильнее всего

* Нормализация данных
Веса обучаются быстрее всего, если соответствующие компоненты входных векторов имеют среднее значение около нуля. Это объясняется тем, что обновление весов в слое происходит пропорционально d*x_i, где d - ошибка по отношению к компоненте веса w_i. Если все x_i > 0 (например), то все веса обновятся в _одном_ направлении sign(d), оказываясь "привязанными" друг к другу.
Приведение к одной дисперсии значений каждого признака для всех примеров способствует тому, что входы всех нейронов входного слоя становятся _одного порядка_, что приводит к лучшему обучению весов (этого слоя).
Присутствие корреляции признаков в данных плохо тем, что а) появляется больше весов, которые нужно обучать, б) появляются направления, в которых градиент равено нулю, что опасно для сходимость. Поэтому данные рекомендуется декоррелировать перед обучением.
+ Приведение к нулевому среднему
+ Декорреляция
+ Приведение к единичной дисперсии


  


