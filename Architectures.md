## Основные идеи популярных архитектур сверточных нейросетей

#### 1. LeNet5
Структура: 2 сверточных слоя (28х28, 14х14), 2 слоя MaxPool (0.5), 2 полносвязных слоя, RBF-слой.
![picture][lenet-arch]

*Слой RBF (Radial basis function)*. Каждый нейрон слоя вычисляет `||z - w_i||`, где z - выход предшествующего слоя (без весов). `w_i` - это векторы-"эталоны", с которыми по норме сравниваются выходы полносвязной части сети. В случае LeNet они были заданы вручную и представляли собой развернутые в вектор мини-представления цифр размера 2*47:

![picture][lenet-rbf]

Интуиция: заставить полносвязную часть сети выучивать представления цифр, а не абстрактные признаки (не быть "черным ящиком").

[lenet-arch]: https://www.researchgate.net/profile/Haohan_Wang/publication/282997080/figure/fig10/AS:305939199610894@1449952997905/Figure-10-Architecture-of-LeNet-5-one-of-the-first-initial-architectures-of-CNN.png
[lenet-rbf]: https://pp.userapi.com/c836322/v836322596/36acb/2fDPME0MXhY.jpg

#### 2. AlexNet
Структура: 5 сверточных слоев (11x11, 5x5, 3x3) с ReLU-активацией, 3 полносвязных слоя, распараллеливание на 2 машины.

![picture][alexnet-arch]

*Local-response normalization*. Значения активаций после применения сверток нормируются по n соседним (по "глубине") фильтрам. 

![picture][alexnet-lrn]

Интуиция: соревнование между соседними ячейками feature maps по аналогии с биологическими нейронами (и по некоторой аналогии с Local Contrast Normalization, когда нормирование производится по окрестности того же фильтра).

[alexnet-arch]: https://pp.userapi.com/c836322/v836322596/36ad3/eMoNe-fsnZ0.jpg
[alexnet-lrn]: https://qph.ec.quoracdn.net/main-qimg-c1dade99f98a8d843e235ca836b7b51e

#### 3. Network-in-Network (NiN)
Структура: 3 "MLP-свертки" + выходной слой с глобальным усреднением.

![picture][nin-arch]

*MlpConv*. Вместо обычных фильтров в качестве скользящего окна для свертки используются небольшие много-уровневые перцептроны (MLP). На вход такого MLP поступает 3d-тензор, на выходе получается вектор, далее выходные векторы объединяются в слой.
Интуиция: MLP является универсальным аппроксиматором, т.е. позволяет вычислять гораздо более сложные функции, чем обычная свертка с фильтром.

*Global Average Pooling*. Вместо стандартного `softmax`-слоя + функции потерь выход сети выглядит следующим образом: предпоследний слой содержит столько feature maps, сколько классов нужно предсказывать модели, а последний слой состоит из стольких же нейронов, каждый из который выдает среднее значение соответствующего feature map с предпоследнего слоя.

![picture][nin-gap]

Интуиция: заставляем сеть выучивать присущие для каждого класса признаки, выраженность которых в итоге позволяет сделать вывод о принадлежности к тому или иному классу. К тому же, по сравнению с обычным полносвязным слоем, получается гораздо меньше весов в сети, что работает как некий регуляризатор.

[nin-arch]: https://pp.userapi.com/c836322/v836322596/36adb/aNIsVmHVtEw.jpg
[nin-gap]: https://pp.userapi.com/c836322/v836322596/36ae2/eD7VfiIAEF4.jpg

#### 4. VGG
Структура: Свертки размером 3х3 (также 1х1 в некоторых модификациях), от 8 до 16 сверточных слоев + 3 полносвязных:

![picture][vgg-arch]

*Слои 3х3*. Можно показать, что последовательное применение двух сверток размера 3x3 эквивалентно применению одной свертки 5x5. Однако, в случае с маленькими фильтрами а) появляется возможность дополнительно вставить нелинейность между ними б) значительно уменьшается общее число весов для обучения.

*Свертка с 1x1*. Свертка с фильтром размера `d x 1 x 1` позволяет применять линейное преобразование к feature maps без изменения размерности. Получается, что можно избавиться от полносвязных слоев, заменив их на свертки 1х1, тогда сеть будет полностью "сверточной", но уметь делать линейные преобразования. Кроме того, можно искусственно уменьшать глубину feature maps (но это другая история).

[vgg-arch]: https://pp.userapi.com/c836322/v836322596/36af7/NMBEFwtWjEA.jpg

#### 5. GoogLeNet (Inception)
Мотивация: совместить возможности dense-вычислений со sparse-структурой сверточной сети. Для поддержания вычислительной эффективности использовать трюки по искусственному понижению размерности промежуточных представлений.

Структура:

![picture][inception-str]

![picture][inception-arch]

*Блок Inception*. В качестве основных строительных блоков архитектуры сети используется т.н. inception-блок. Он состоит из нескольки сверток разного размера (читай: для разного масштаба), вычисляемых по входному представлению, результат которых конкатенируется и подается на выход. Если использовать наивный подход, то в прогрессии должен увеличиваться размер представления от слоя к слою, быстро останавливая обучение. Для борьбы с этим перед свертками 3х3 и 5х5 используются понижающие размерность банки фильтров меньшей глубины со свертками 1х1. Кроме этого, на выход подаётся результат maxpooling-а входа блока.

![picture][inception-inception]

*Auxiliary classifiers*. Для того, чтобы сделать извлекаемые в процессе прохождения сети признаки более дискриминативными (читай: полезными, выделяющими ключевые особенности), к промежуточным слоям прицепляются блоки классификаторов, выход которых дает свой вклад в глобальную функцию потерь модели. Благодаря этому штрафуются "плохие признаки", появляющиеся на промежуточных слоях. После обучения модели эти дополнительные классификаторы просто выключаются. По аналогии с сетью NiN для промежуточных классификаторов используется average-pooling, за которым, тем не менее, следует полносвязный слой.

![picture][inception-auxiliary]

[inception-arch]: https://adeshpande3.github.io/assets/GoogLeNet.png
[inception-str]: https://pp.userapi.com/c836322/v836322596/36c70/AavCXPtN46c.jpg
[inception-inception]: https://pp.userapi.com/c836322/v836322596/36c77/5oXIsPeJ08o.jpg
[inception-auxiliary]: http://upload-images.jianshu.io/upload_images/2228224-b473d26009856550.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/500
