## Основные идеи популярных архитектур сверточных нейросетей

#### 1. LeNet
Структура: 2 сверточных слоя (28х28, 14х14), 2 слоя MaxPool (0.5), 2 полносвязных слоя, RBF-слой.
![picture][lenet-arch]

*Слой RBF (Radial basis function)*. Каждый нейрон слоя вычисляет `||z - w_i||`, где z - выход предшествующего слоя (без весов). `w_i` - это векторы-"эталоны", с которыми по норме сравниваются выходы полносвязной части сети. В случае LeNet они были заданы вручную и представляли собой развернутые в вектор мини-представления цифр размера 2*47:

![picture][lenet-rbf]

Интуиция: заставить полносвязную часть сети выучивать представления цифр, а не абстрактные признаки (не быть "черным ящиком").

[lenet-arch]: https://www.researchgate.net/profile/Haohan_Wang/publication/282997080/figure/fig10/AS:305939199610894@1449952997905/Figure-10-Architecture-of-LeNet-5-one-of-the-first-initial-architectures-of-CNN.png
[lenet-rbf]: https://pp.userapi.com/c836322/v836322596/36acb/2fDPME0MXhY.jpg

#### 2. AlexNet
Структура: 5 сверточных слоев (11x11, 5x5, 3x3) с ReLU-активацией, 3 полносвязных слоя, распараллеливание на 2 машины.

![picture][alexnet-arch]

*Local-response normalization*. Значения активаций после применения сверток нормируются по n соседним (по "глубине") фильтрам. 

![picture][alexnet-lrn]

Интуиция: соревнование между соседними ячейками feature maps по аналогии с биологическими нейронами (и по некоторой аналогии с Local Contrast Normalization, когда нормирование производится по окрестности того же фильтра).

[alexnet-arch]: https://pp.userapi.com/c836322/v836322596/36ad3/eMoNe-fsnZ0.jpg
[alexnet-lrn]: https://qph.ec.quoracdn.net/main-qimg-c1dade99f98a8d843e235ca836b7b51e

#### 3. Network-in-Network (NiN)
Структура: 3 "MLP-свертки" + выходной слой с глобальным усреднением.

![picture][nin-arch]

*MlpConv*. Вместо обычных фильтров в качестве скользящего окна для свертки используются небольшие много-уровневые перцептроны (MLP). На вход такого MLP поступает 3d-тензор, на выходе получается вектор, далее выходные векторы объединяются в слой.
Интуиция: MLP является универсальным аппроксиматором, т.е. позволяет вычислять гораздо более сложные функции, чем обычная свертка с фильтром.

*Global Average Pooling*. Вместо стандартного `softmax`-слоя + функции потерь выход сети выглядит следующим образом: предпоследний слой содержит столько feature maps, сколько классов нужно предсказывать модели, а последний слой состоит из стольких же нейронов, каждый из который выдает среднее значение соответствующего feature map с предпоследнего слоя.

![picture][nin-gap]

Интуиция: заставляем сеть выучивать присущие для каждого класса признаки, выраженность которых в итоге позволяет сделать вывод о принадлежности к тому или иному классу. К тому же, по сравнению с обычным полносвязным слоем, получается гораздо меньше весов в сети, что работает как некий регуляризатор.

[nin-arch]: https://pp.userapi.com/c836322/v836322596/36adb/aNIsVmHVtEw.jpg
[nin-gap]: https://pp.userapi.com/c836322/v836322596/36ae2/eD7VfiIAEF4.jpg

#### 4. VGG
Структура: Свертки размером 3х3 (также 1х1 в некоторых модификациях), от 8 до 16 сверточных слоев + 3 полносвязных:

![picture][vgg-arch]

*Слои 3х3*. Можно показать, что последовательное применение двух сверток размера 3x3 эквивалентно применению одной свертки 5x5. Однако, в случае с маленькими фильтрами а) появляется возможность дополнительно вставить нелинейность между ними б) значительно уменьшается общее число весов для обучения.
*Свертка с 1x1*. Свертка с фильтром размера `d x 1 x 1` позволяет применять линейное преобразование к feature maps без изменения размерности. Получается, что можно избавиться от полносвязных слоев, заменив их на свертки 1х1, тогда сеть будет полностью "сверточной", но уметь делать линейные преобразования. Кроме того, можно искусственно уменьшать глубину feature maps (но это другая история).

[vgg-arch]: https://pp.userapi.com/c836322/v836322596/36af7/NMBEFwtWjEA.jpg
